---
title: 深度学习基本知识介绍、CNN/RNN/LSTM/GRU神经网络原理
date: 2019-03-09 13:26:51
tags:
- 深度学习原理推导
categories:
- 深度学习原理推导
mathjax: true
---

# 机器学习定义
对于某类任务T和性能度量P，一个计算机程序被认为可以从经验E中学习是指，通过经验E改进后，它在任务T上由性能度量P衡量的性能有所提升。
# 训练误差/泛化误差/假设空间
训练误差：模型在训练集上的误差。
泛化误差：模型在测试集上的误差。
假设空间：算法可以选择的函数的总数量。
# 为什么利用训练集生成的模型可以用来预测测试集的数据？
我们假设数据之间是相互独立的（即数据独立同分布假设），而数据都是由某个概率分布函数生成，因此在训练数据（已知）上表现很好的算法，在测试数据（未知）上依然能够表现得很好。
# Error来源分析
Error = bias + variance + noise。
bias描述的是模型在训练集上拟合的好不好:拟合的好就是low bias，模型就较为复杂，参数较多，容易过拟合，使得模型在测试集上的预测具有high variance； 拟合的不好就是high bias，模型较为简单，参数较少，容易欠拟合，但是这样的模型由于对数据变化不那么敏感（不管是训练数据还是测试数据），因此在测试集上的预测具有low variance。
因此，想让模型的训练误差小（low bias）就要更复杂的模型，但容易造成过拟合，在测试样本上出现high variance；想让模型的测试误差小（low variance），则模型要简单一点，这样泛化能力才强，但容易出现high variance。
# 奥卡姆剃刀原理
若有多个假设与观察一致，则选最简单的那个假设。
# 没有免费的午餐定理
在所有可能的数据生成分布上平均之后，每一个分类算法在未事先观测的点上都有相同的错误率。通俗来说，就是针对某一域的所有问题，所有算法的期望性能是相同的。
假如学习算法La在某些问题上比学习算法Lb要好， 那么必然存在另一些问题，在这些问题中Lb比La表现更好。这里说的表现好就是模型泛化能力更强。因此，脱离具体的问题，考虑所有潜在的问题，则所有的学习算法都一样好。要谈论算法的相对优劣，必须要针对具体的学习问题，在某些问题上表现好的学习算法在另一些问题上可能不尽如人意。
# 上采样与下采样
在机器学习中，采样用来训练的数据集时，如果抛弃一部分样本，称为下采样，如果一部分样本重复采样，称为上采样/过采样；在图像识别中，缩小图像称为下采样，放大图像称为上采样。
# dropout层作用
在隐藏层卷积和池化过后，按一定的比例保留神经元的值，其他神经元值置为0，因此本次迭代反向传播时，这些值为0的神经元的梯度会被计算为0，它们的权重本次迭代时不会更新。而非零值神经元，它们的值要除以这个比率，相当于值被放大了，因此计算出来的梯度也放大了，梯度下降时权重更新的幅度变大了。
dropout层只在训练时开启，使用dropout层训练时可以减轻过拟合，但会延长训练的时间。在测试时，不要打开dropout层。
# 模型评估方法:Holdout检验、k折交叉验证、自助法
**Holdout检验:**
将原始的样本集合随机划分成训练集和验证集两部分。一般按训练集:验证集=7:3划分。
**k折交叉验证:**
将全部样本划分成k个大小相等的样本子集；使用k次，第i次把第i个子集作为验证集，其余所有子集作为训练集，进行模型的训练和评估；最后把k次评估指标的平均值作为最终的评估指标。在实际实验中，k经常取10。
**自助法:**
每一次训练迭代（iteration）都有放回地从一个总数n的样本中抽取m个样本（batch size），进行n/m次迭代后称为一次epoch（即我们一共用了整个数据集大小数量的样本进行了训练）。
在自助法的采样过程中，对n个样本进行n次自助抽样，当n趋于无穷大时，最终有多少数据从未被选择过？
$$
\lim_{n \rightarrow \infty}(1-\frac{1}{n})^{n}=\lim_{n \rightarrow \infty} \frac{1}{(1+\frac{1}{n-1})^{n}}=\frac{1}{\mathrm{e}} \approx 0.368
$$
# 判别模型和生成模型
生成模型：学习得到联合概率分布P（x,y），即特征x和标记y共同出现的概率，然后求条件概率p（y|x）。如贝叶斯分类器，注意贝叶斯分类器实际是通过求p（x|c）和p（c）来求得p（x,c）。
判别模型：学习得到条件概率分布P（y|x），也就是特征x出现的情况下标记y的出现概率。
生成模型需要的数据量比较大，能够较好地估计概率密度；而判别模型对数据样本量的要求没有那么多。由生成模型可以得到判别模型，但由判别模型得不到生成模型。
# L1正则化/L2正则化
**适定问题：**
即指定解满足下面三个要求的问题:解是存在的；解是唯一的；解连续依赖于定解条件，即解是稳定的。这三个要求中，只要有一个不满足，则称之为不适定问题。
**正则化方法:**
求解不适定问题的普遍方法是用一组与原不适定问题相“邻近”的适定问题的解去逼近原问题的解,这种方法称为正则化方法。
正则化就是对最小化经验误差函数上加约束，这样的约束可以解释为先验知识（正则化参数等价于对参数引入先验分布）。约束有引导作用，在优化误差函数的时候倾向于选择满足约束的梯度减少的方向，使最终的解倾向于符合先验知识。
L1正则化项即所有权重的绝对值之和。L2正则化项即所有权重的平方之和。当我们在损失函数中加入正则化项时还要乘以一个λ。
L1 正则化可以产生更稀疏的权值矩阵，可以用于特征选择，同时一定程度上防止过拟合；L2正则化主要用于防止模型过拟合。
L1 正则化适用于特征之间有关联的情况；L2 正则化适用于特征之间没有关联的情况。
## L1与L2正则化的区别:优化角度
L1正则化的最优化问题等价于:
$$
\min_{w} L(w)=\min_{w}\left(E_{D}(w)+\lambda \sum_{i=1}^{n} w_{i} |\right)
$$
可写为凸优化问题:
$$
\min_{w} E_{D}(w)
$$
$$
\sum_{i=1}^{n}\left|w_{i}\right| \leq C
$$
其中C与正则化参数λ成反比关系。也就是在一个限定的区域内找到:
$$
\min E_{D}(w)
$$
考虑只有w1和w2的情况时，如果我们画出图来，L1正则项的区域就是一个以原点为中心点，旋转了45度角的正方形。我们的E_{D}(w)最小值在一个圆内，这个圆在正方形的右上角，且与正方形相交。在交点处，切向量始终指向w2轴，所以L1正则化容易使参数w2为0，即特征稀疏化。

L2正则化的最优化问题等价于:
$$
\min_{w} L(w)=\min_{w}\left(E_{D}(w)+\lambda \sum_{i=1}^{n} w_{i}^{2}\right)
$$
可写为凸优化问题:
$$
\min_{w} E_{D}(w)
$$
$$
\sum_{i=1}^{n} w_{i}^{2} \leq C
$$
其中C与正则化参数λ成反比关系。也就是在一个限定的区域内找到:
$$
\min E_{D}(w)
$$
考虑只有w1和w2的情况时，如果我们画出图来，L2正则项的区域就是一个以原点为圆心的圆。我们的E_{D}(w)最小值在一个圆内，这个圆在L2圆的右上角，且与其相交。在交点处，切向量相切与正则化圆，我们进行梯度下降时总是沿着正则化圆移动直接其点离E_{D}(w)最近，此时参数w1和w2都能取到比较小的值，但参数不为0。
## L1与L2正则化的区别:梯度角度
L1正则化的损失函数:
$$
L(\mathrm{w})=E_{D}(w)+\frac{\lambda}{n} \sum_{i=1}^{n}\left|w_{i}\right|
$$
$$
\frac{\partial L(\mathrm{w})}{\partial w}=\frac{\partial E_{D}(w)}{\partial w}+\frac{\lambda \operatorname{sgn}(w)}{n}
$$
$$
w^{\prime}=w-\eta \frac{\partial L(\mathrm{w})}{\partial w}
$$
$$
w^{\prime}=w-\frac{\eta \lambda \operatorname{sgn}(w)}{n}-\frac{\eta \partial E_{D}(w)}{\partial w}
$$
其中η为学习率。Sgn 函数有如下返回值：大于零时返回1，等于零时返回0，小于零时返回-1。
可以看出，当w大于0时，更新的参数w变小；当w小于0时，更新的参数w变大；所以，L1正则化容易使参数变为0，即特征稀疏化。

L2正则化的损失函数:
$$
L(w)=E_{D}(w)+\frac{\lambda}{2 n} \sum_{i=1}^{n} w_{i}^{2}
$$
$$
\frac{\partial L(w)}{\partial w}=\frac{\partial E_{D}(w)}{\partial w}+\frac{\lambda w}{n}
$$
$$
\mathrm{w}^{\prime}=w-\eta \frac{\partial L(w)}{\partial w}
$$
$$
w^{\prime}=w-\frac{\eta \lambda w}{n}-\frac{\eta \partial E_{D}(w)}{\partial w}
$$
$$
w^{\prime}=\left(1-\frac{\eta \lambda}{n}\right) w-\frac{\partial E_{D}(w)}{\partial w}
$$
注意这项:
$$
\left(1-\frac{\eta \lambda}{n}\right) w
$$
当w趋向于0时，参数减小的非常缓慢，因此L2正则化使参数减小到很小的范围，但不为0。
# 深度学习训练、调参Tips
**防止过拟合方法:**
如果在训练集上性能较好，而在测试集上性能不好，那么这种情况就叫做过拟合。
防止过拟合可以使用早停、正则化、dropout、批标准化、残差网络等方法。
**早停:**
我们可以在训练过程中每隔固定steps数运行测试检查验证集预测准确率来及早发现过拟合的发生，这又叫早停。
**dropout使用注意事项:**
在训练时使用dropout层可以有效地减少过拟合，但是会延长训练过程，因为dropout后每轮更新的参数数量变少了（但更新的参数的梯度变大了，更新的更快），需要更多轮训练迭代。在测试时，我们应当关闭dropout层，使所有神经元都参与运算。
**梯度消失:**
梯度消失现象是由于反向传播的梯度是链式法则求导，如果激活函数使用类似sigmoid的激活函数，求导函数的特性会使得最初的几层的梯度变得很小，权重参数更新很慢。这也是激活函数从sigmoid函数改动relu函数的原因。
**训练后期反复震荡现象:**
训练过程中为了避免出现固定学习率在训练后期过大而导致在局部最优解两边反复震荡的情况，我们可以使用具有自适应学习率的优化算法，比如Adagrad。
**梯度爆炸:**
梯度爆炸即不同层之间的梯度（值大于1.0）相乘，使得权重更新时呈指数级爆炸增长。在极端情况下，权重的值变得非常大，以至于溢出，导致NaN值。
我们可以使用梯度截断、正则化、批标准化解决梯度爆炸现象。
# 批标准化（Batch Normalization）
论文:Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift
论文地址:https://arxiv.org/pdf/1502.03167.pdf 。
深层神经网络在每一层做线性变换和激活后，得到的值随着网络深度加深或在训练过程中，其分布会逐渐发生偏移或者变动，之所以训练收敛慢，一般是整体分布逐渐往非线性函数（如sigmoid函数）的取值区间的上下限两端靠近，这会导致反向传播时最后几层神经网络的梯度越来越小，出现梯度消失的现象。
**批标准化就是对每层神经元的原始激活值通过标准化规整成服从正态分布的值，把线性变换后得到的值的分布强行拉回到均值为0方差为1的标准正态分布，这样使得这些值落在非线性函数中对输入比较敏感的区域，这样输入值的小变化就会导致损失函数较大的变化，梯度就会变大，避免梯度消失的问题。而且梯度变大意味着学习收敛速度快，能大大加快训练速度。**
我们要注意一点的是，对数据进行批标准化一般都将其分布拉回标准正态分布上，但有时训练集数据并不服从正态分布，这同样会导致网络的性能下降。为解决该问题，我们引入两个新的参数γi,βi来学习原有的分布。γi和βi是在训练时网络自己学习得到的，它们用来恢复要学习的数据集特征。 
**批标准化计算过程:**
首先将原始激活值规整到均值为0，方差为1的正态分布范围内。其中，ai为某个神经元原始激活值,μ和σi分别为期望和标准差，要计算期望和标准差，我们必须确定一个数据范围。
$$
\tau=\frac{a_{i}-\mu}{\sigma_{i}}
$$
接下来根据我们采用的神经网络分两种情况:
* 如果是全连接神经网络，对于我们最常见的mini-batch形式的训练，假如某个mini-batch有n个样本，每个样本在某层的某个神经元k上都会产生一个原始激活值，mini-batch中n个样本在通过这个神经元k时产生n个激活值，那么我们就用这n个激活值来计算对这个神经元k的激活值的期望和方差；
* 如果是CNN神经网络，对于我们最常见的mini-batch形式的训练，假如某个mini-batch有n个样本，对于某层中的单个卷积核，n个样本通过这个卷积核时产生了N个特征图，假设每个特征图长X宽=RXL，那么对于这个卷积核上的所有特征值，它们的期望和标准差就由这RXLXN个值产生。
  然后我们要在模型训练过程中随其他权重参数一起，学习两个调节因子γi和βi，对上面规整后的值进行微调。因为经过第一步操作后，批标准化有可能降低神经网络的非线性表达能力，所以会以此方式来补偿批标准化操作后的神经网络的表达能力。
  之后，我们要学习γi和βi两个调节因子，用来恢复要学习的数据集特征。γi和βi的学习是自动的，同网络中其他权重参数一样，是通过反向传播在训练过程中自动学习的。 下式中，ai_norm为经过标准化操作后的值。
$$
a_{i}^{norm}=\gamma_{i} \cdot \tau+\beta_{i}
$$
# 特征标准化
特征标准化（Feature Scaling），就是让不同的特征值具有相同的缩放程度。
**z-score标准化:**
$$
y_{i}=\frac{x_{i}-\overline{x}}{s}
$$
$$
\overline{x}=\frac{1}{n} \sum_{i=1}^{n} x_{i}, \quad s=\sqrt{\frac{1}{n-1} \sum_{i=1}^{n}\left(x_{i}-\overline{x}\right)^{2}}
$$
**min-max标准化:**
$$
X_{n o m}=\frac{X-X_{\min }}{X_{\max }-X_{\min }}
$$
# 为什么交叉熵损失可以提高具有 sigmoid 和 softmax 输出的模型的性能，而使用均方误差损失则会存在很多问题
因为sigmoid函数求导后是
$$
f^{\prime}(x)=f(x)(1-f(x))
$$
如果损失函数是平方误差函数，那么求导后的式子易导致更新梯度变得很小。
而使用交叉熵函数作损失函数后，与softmax函数反向传播求导时的式子就很简单，而且不会造成梯度消失。
# softmax函数在框架中计算时的优化
softmax函数公式:
$$
S_{i}=\frac{e^{i}}{\sum_{j} e^{j}}
$$
在机器学习中，使用softmax函数时输入数据往往是神经网络最后一层的输出值，通过softmax函数可以转化成所有神经元输出的值的和为1，因此可以看成概率来理解，这样我们就可以用来进行多分类。
实际计算时为了防止上溢，将e的x次方的分子分母上的x都减去一个xmax。防止下溢可在分母中加上一个小的常数即可。
# 激活函数的作用
如果没有激活函数，那么无论多少层网络，最终得到的函数都可以表示成一个线性组合。只有加入激活函数后，才能得到复杂的多层非线性模型，拟合能力更加强大。
神经网络具有至少一个非线性隐藏层，那么只要给予网络足够数量的隐藏单元，它就可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的函数。如果不使用非线性激活函数，那么每一层输出都是上层输入的线性组合。
# relu函数的作用
首先如果不用非线性的激励函数（其实相当于激励函数是f(x) = x），则在这种情况下神经网络的每一层输出都是上层输入的线性函数，我们可以很容易地将其化简成只有一层y=wx+b的形式，那么这种情况下神经网络的隐藏层有没有效果都一样。
relu=max（0，x），当输出>0时，反向传播链式求导时这个函数导数为1，不容易发生梯度消失；当输出<0时，反向传播求导时这个函数导数为0，即此时参数中反向传播时只要含有该项的参数，其梯度都为0，该轮反向传播时参数不更新。
另外，有relu函数时神经网络训练完成后为0的神经元变多，这增大了神经网络的稀疏性。神经网络的稀疏性越大，提取出来的特征就约具有代表性，泛化能力越强。即得到同样的效果，真正起作用的神经元越少，网络的泛化性能越好。
另外，每一次relu函数相当于是一个二段的线性分段函数，多次relu后就变成了一个很多段的线性分段函数（每次分段点不一定相同），这样就可以近似地拟合一个非线性的函数。
**缺点:**
输出不是0均值；
有些神经元可能永远无法激活（参数一直为0）。
# 反向传播的过程
梯度下降法在每轮迭代时计算损失函数对所有权重参数的梯度（偏导数），然后用梯度更新权重参数，然后下一轮迭代时重新计算损失函数值。比较损失函数增大还是缩小了，将权重参数朝着损失函数值缩小的方向更新。
# CNN神经网络原理概述
卷积神经网络（Convolutional Neural Networks）由许多个卷积层-池化层-dropout层这样的三层小结构串联而成（注意池化和dropout层都不一定是必须的），在网络的末尾往往还要接上1-2层全连接层神经网络，最后接输出层。
卷积层中最重要的概念就是卷积计算，一个卷积层中有多个卷积核（比如32、64、128）,同一个卷积层中的卷积核尺寸都相同，卷积核依照卷积层定义中的步长（行上和列上都有步长），从一个输入的图片样本数据矩阵的左上角开始依次扫过，每次都从图片样本数据矩阵中提取一个和卷积核大小相同的矩阵，称之为感受域。卷积核与这个矩阵做矩阵同样位置上元素相乘的乘积再求和的计算，最后再与卷积层中的bias变量相加，就得到了feature map中的一个位置上的特征值。一个卷积核将一个图片样本数据完整地扫完后，就得到一个完整的feature map。一个卷积层中有多少个卷积核，本层卷积结束后就能得到多少层feature map。
池化层的重要作用是浓缩feature map，减少feature map的数据量，同时又保留feature map中最有表现力的特征值，这样还可以减轻过拟合。以最常见的最大池化为例，filter窗口往往设为2x2，步长一般和窗口边长一样也为2，那么最大池化就是在2x2的窗口中选取一个最大值保留下来。窗口按照步长将整个feature map扫完，最后得到一个长宽均缩小为原来一半的新feature map。
dropout层按照我们设定的比率随机地保留feature map中这个比例数的特征值，而将其他特征值置为0。同时被保留的特征值都要除以这个比率，相当于把这部分特征值都放大了。dropout层可以有效地减轻过拟合，但因为反向传播时特征值为0的神经元权重都不更新，因此也延长了训练时间。
全连接层用来进一步把特征图压缩（压缩前会把特征图拉平为1维向量），多个全连接层最终将特征向量中的元素数量压缩到刚好等于我们需要预测的类别数。最后使用softmax函数将特征向量归一化就可以得到预测结果。
CNN卷积得到的特征图具有空间不变性，特征图上不同位置的特征值与输入样本特征分布一致。CNN网络中卷积层的权值是共享的，卷积层中的所有卷积核中的权重对一个图片样本的矩阵上的所有元素来说是共享的。
# 1x1卷积的作用
输入为6x6x128的特征图时，1x1卷积的形式是1x1x32时，输出为6x6x32。即1x1的卷积核通道数如果少于输出的特征图通道数，就可以减少特征图的通道数，也就是降维。1×1卷积不会改变特征图的height和width，1x1的卷积实际上是将不同channel上同一位置上的特征值进行线性组合变化，实现了跨通道的交互和信息整合。
# CNN神经网络feature map尺寸计算
在每个卷积层中，有多少个卷积核，就有多少层特征图（或者说输出通道数）。对于输入的图片数据，如果是灰度图片，那么输入数据通道数为1；如果是彩色图片，那么输入通道数为3。
卷积后得到feature map边长计算公式： 
$$
h_{out} =\left(\left(h_{in}+p^{*} 2-k\right) / s\right)+1
$$
h（in）为输入图片的长或宽，p为边缘要填充的行或列数，k为卷积核尺寸，s为步长，这里假设的是图片、卷积核、步长在长和宽方向均相等的情况。如果长和宽不等，仍然按上式计算，只要记得是用同样的对应位置上的数据来计算即可。如果算出来不是整数，则取上值整数。
卷积层之后进行pooling后feature maps边长计算公式：
$$
pool1_{h}=((conv1_{h}-k_{p})/s_{p})+1
$$
其中conv1（h）为卷积层输出的特征图尺寸，kp为池化层窗口边长，sp为池化层步长。
**举例：**
假设输入图片尺寸为32X32，使用6个5X5的卷积核，步长为1，pading=0。那么这一层卷积后的feature map尺寸为：
$$
(32-5) / 1+1=28
$$
即卷积层尺寸为28X28。
卷积层后假如pooling层的filter=2X2，步长stride=1，pooling方法为最大池化。那么pooling层后的feature map的边长为：
$$
(28-2) / 2+1=14
$$
即pooling后尺寸为14X14。
# RNN神经网络原理概述
RNN的核心内容就是使用了循环连接，使用循环链接之后，可以使得神经网络拥有动态记忆能力。所谓循环连接，指的就是上一时刻的隐藏单元的输出s（t）也作为下一时刻的隐藏单元的输入，参与下一个时刻对s（t+1）的计算。
RNN能够处理不定长的输入数据，但是在某个时刻的输入还是定长的。RNN的权值同样是共享的，即RNN神经元中的权重对于不同时刻的U、W、V都是共享的。
某一个时刻的RNN神经元包含输入层、隐藏层、输出层。它们各自有一个权重矩阵，分别为U、W、V。在本时刻输入样本数据为x（t），与x（t）相乘的权重为U，上一时刻的隐藏层为s（t-1），与s（t-1）相乘的权重为W，输出为o（t），o（t）由本层记忆与权重矩阵V相乘得到。
如果对RNN网络的架构仍不是很清楚，请在这个网站上查看：http://www.asimovinstitute.org/neural-network-zoo/ 。
**RNN神经网络的计算过程:**
对某一个时刻的单个RNN神经元来说，t时刻的输入向量x（t）与输入层的权重矩阵U相乘，上一时刻的隐藏层输出（即我们所说的记忆）s（t-1）与本时刻的隐藏层权重矩阵W相乘，两者相加再经过激活函数（如sigmoid函数）得到的输出值就是本时刻隐藏层的输出值s（t）。s（t）再与本时刻输出层权重矩阵V相乘，得到的结果再经过激活函数（通常是softmax函数）得到的输出值就是本时刻的输出值o（t）。
每一个时刻t都会保存两个值：隐藏层的输出值（即记忆）s（t）和输出层输出值o（t）。其中s（t）会参与下一个时刻的计算。
需要注意的是，每一步的输出并不是必须的。假如我们把图片的每一行像素作为每个时刻的输入，最后我们要预测图片输入哪个类，那么我们只关心最后一个时刻的输出。
用公式来表示计算过程如下：
$$
s_{t}=f_{1}\left(U x_{t}+W_{S_{t-1}}\right)
$$
$$
o_{t}=f_{2}\left(V_{S_{t}}\right)
$$
**RNN的训练过程：**
对RNN的训练，也使用反向传播（backpropagation）算法，但反向传播链式求导计算时基于的是时刻（RNN中的不同时刻就类似CNN中的不同层），被称为BPTT算法。
但要注意一点的是，由于RNN的参数被所有时刻共享，因此当前时刻的输出不仅要依靠当前时刻所计算出来的梯度，还依赖于之前所有时刻上计算得到的梯度。即下面的计算公式：
$$
\frac{\partial E}{\partial W}=\sum^{t} \frac{\partial E_{t}}{\partial W}=\sum_{k=0}^{t} \frac{\partial E_{t}}{\partial n e t_{t}} \frac{\partial n e t_{t}}{\partial s_{t}}\left(\prod_{j=k+1}^{t} \frac{\partial s_{t}}{\partial s_{k}}\right) \frac{\partial s_{k}}{\partial W}
$$
如时刻t=4时，还需要向后传递三步，已经后面的三步都需要加上各种梯度。
关于BPTT算法，可看这篇文章：On the difficulty of training recurrent neural networks，文章地址: http://proceedings.mlr.press/v28/pascanu13.pdf 。
基于BPTT来训练RNN，由于其训练存在递归的问题，如果RNN的递归深度过长（一般超过十个时刻就难以为继），将会带来所谓的梯度消失或梯度爆炸问题。
# LSTM神经网络原理概述
LSTM（Long Short Memory Network）即长短时记忆网络。它是为了克服RNN无法很好地处理长期记忆而提出的。由于RNN的隐藏层输出（即所谓的长期记忆）s（t）中每一个时刻都要经过激活函数sigmoid处理，而sigmoid函数求导后形式为sigmoid（1-sigmoid），这会导致对于靠前的时刻梯度快速减小，也就是梯度消失现象。为了避免梯度消失，人们对RNN网络进行了改进，实现了一种较为复杂的累加形式的反向传播，这就是LSTM网络。
对于某一个时刻的LSTM网络单个神经元来说，它包含三个结构：遗忘门、输入门和输出门。遗忘门负责决定保留多少上一时刻的单元记忆到当前时刻的单元记忆；输入门负责决定保留多少当前时刻的输入到当前时刻的单元记忆；输出门负责决定当前时刻的单元记忆有多少输出。
**LSTM网络计算过程:**
对于某一个时刻的LSTM网络单个神经元：
每个时刻的LSTM单个神经元有三个输入，即上一时刻的长时单元记忆C（t-1）、上一时刻单元的输出h（t−1）和当前时刻的输入x（t）。输出有两个值，C（t）代表了长时记忆，h（t）则代表了短时记忆。
遗忘门是上一时刻输出h（t−1）和本单元输入x（t）做矩阵乘法，再左乘遗忘门权重矩阵W（f），最后加上遗忘门的偏置b（f），将得到的值输入sigmoid函数后得到一个[0,1]内的激活值，来控制上一单元状态被遗忘的程度。
其中中括号表示两个向量相连合并。设输入层维度为dx，隐藏层维度为dh，上面的状态维度为dc，则W（f）的维度为dc（dh+dx）。
$$
f_{t}=\sigma(W_{f}\cdot [h_{t-1}, x_{t}]+b_{f})
$$
输入门的计算分为两部分：i（t）的计算与遗忘门的计算方法一致，只是权重全部换成了输入门i中的权重；头顶带波浪的C（t）的计算与遗忘门的计算也基本一致，权重全部换成了输入门C中的权重，但激活函数变为tanh函数，产生一个[-1,1]之间的值。
$$
i_{t}=\sigma(W_{i}\cdot [h_{t-1}, x_{t}]+b_{i})
$$
$$
\tilde C_{t}=\tanh (W_{C}\cdot [h_{t-1}, x_{t}]+b_{C})
$$
下面就是更新长期记忆C（t）的计算，当前时刻的单元长期记忆C（t）由遗忘门输入和上一时刻状态的积加上输入门两部分的积组成。
$$
C_{t}=f_{t}\cdot  C_{t-1}+i_{t}\cdot \tilde C_{t}
$$
输出门的计算方式和遗忘门仍相同，只是权重全部换成了输出门O中的权重，计算出的o（t）用来控制当前时刻的单元记忆有多少输出。最后本时刻的短期记忆h（t）即为o（t）与tanh（C（t））的乘积。
$$
o_{t}=\sigma(W_{o}\cdot [h_{t-1}, x_{t}]+b_{o})
$$
$$
h_{t}=o_{t} \cdot \tanh (C_{t})
$$
# GRU神经网络原理概述
GRU神经网络（Gated Recurrent Unit，门控循环单元神经网络）是LSTM神经网络的一个变体。GRU神经网络只有两个门，分别是更新门和重置门。GRU神经网络实际上是将LSTM神经网络中的遗忘门和输入门合成了一个单一的更新门。
重置门用于控制忽略前一时刻的记忆的程度，重置门的值越小说明忽略得越多。更新门用于控制前一时刻的记忆被带入到当前状态中的程度，更新门的值越大说明前一时刻的记忆带入越多。
**具体计算过程如下:**
rt是重置门，zt是更新门，ht-1是上一时刻传来的长时记忆，xt是本时刻输入的信息。rt和zt首先产生两个0到1之间的阈值。
$$
r_{t}=\sigma\left(W_{r} \cdot\left[h_{t-1}, x_{t}\right]\right)
$$
$$
z_{t}=\sigma\left(W_{z} \cdot\left[h_{t-1}, x_{t}\right]\right)
$$
然后计算头顶带波浪号的ht（候选隐含记忆），计算时rt这个重置门阈值控制了ht-1带入到带波浪ht（候选隐含记忆）中的信息量，重置门值越小则上一时刻的长时记忆ht-1的信息被丢弃的越多。然后用带波浪ht和ht-1更新本时刻的长时记忆ht。zt就是更新门，更新门可以控制过去的长时记忆ht-1在当前时刻的重要性。如果更新门一直近似1，过去的隐含状态将一直通过时间保存并传递至当前时刻。
$$
\tilde h_{t}=tanh((W_{\tilde{h}}) \cdot[r_{t} \cdot h_{t-1}, x_{t}])
$$
$$
h_{t}=(1-z_{t}) \cdot h_{t-1}+z_{t} \cdot (\tilde h_{t})
$$
其中[]表示两个向量相连接，点号表示矩阵元素相乘。