---
title: 深度学习基本知识介绍、CNN/RNN/LSTM/GRU神经网络原理
date: 2019-03-09 13:26:51
tags:
- 深度学习原理推导
categories:
- 深度学习原理推导
mathjax: true
---

# 机器学习定义
对于某类任务T和性能度量P，一个计算机程序被认为可以从经验E中学习是指，通过经验E改进后，它在任务T上由性能度量P衡量的性能有所提升。
# 训练误差/泛化误差/假设空间
训练误差：模型在训练集上的误差。
泛化误差：模型在测试集上的误差。
假设空间：算法可以选择的函数的总数量。
# 为什么利用训练集生成的模型可以用来预测测试集的数据？
我们假设数据之间是相互独立的（即数据独立同分布假设），而数据都是由某个概率分布函数生成，因此在训练数据（已知）上表现很好的算法，在测试数据（未知）上依然能够表现得很好。
# Error来源分析
Error = bias + variance + noise。
bias描述的是训练样本在模型上拟合的好不好:拟合的好就是low bias，模型就较为复杂，参数较多，容易过拟合，使得测试样本在模型上的预测具有high variance； 拟合的不好就是high bias，模型较为简单，参数较少，容易欠拟合，但是这样的模型由于对数据变化不那么敏感（不管是训练数据还是测试数据），因此在测试样本上的输出具有low variance。
因此，想让模型的训练误差小（low bias）就要更复杂的模型，但容易造成过拟合，在测试样本上出现high variance；想让模型的测试误差小（low variance），则模型要简单一点，这样泛化能力才强，但容易出现high variance。
# 奥卡姆剃刀原理
若有多个假设与观察一致，则选最简单的那个假设。
# 没有免费的午餐定理
在所有可能的数据生成分布上平均之后，每一个分类算法在未事先观测的点上都有相同的错误率。通俗来说，就是针对某一域的所有问题，所有算法的期望性能是相同的。
假如学习算法La在某些问题上比学习算法Lb要好， 那么必然存在另一些问题，在这些问题中Lb比La表现更好。这里说的表现好就是模型泛化能力更强。因此，脱离具体的问题，考虑所有潜在的问题，则所有的学习算法都一样好。要谈论算法的相对优劣，必须要针对具体的学习问题，在某些问题上表现好的学习算法在另一些问题上可能不尽如人意。
# 上采样与下采样
缩小图像称为下采样，放大图像称为上采样。
# dropout层作用
在隐藏层卷积和池化过后，按一定的比例保留神经元的值，其他神经元值置为0，因此本次迭代反向传播时，这些值为0的神经元的梯度会被计算为0，它们的权重本次迭代时不会更新。而非零值神经元，它们的值要除以这个比率，相当于值被放大了，因此计算出来的梯度也放大了，梯度下降时权重更新的幅度变大了。
dropout层只在训练时开启，使用dropout层训练时可以减轻过拟合，但会延长训练的时间。在测试时，不要打开dropout层。
# 模型评估方法:Holdout检验、k折交叉验证、自助法
**Holdout检验:**
将原始的样本集合随机划分成训练集和验证集两部分。一般按训练集:验证集=7:3划分。
**k折交叉验证:**
将全部样本划分成k个大小相等的样本子集；依次遍历这k个子集，每次把当前子集作为验证集，其余所有子集作为训练集，进行模型的训练和评估；最后把k次评估指标的平均值作为最终的评估指标。在实际实验中，k经常取10。
**自助法:**
每一次训练迭代（iteration）都有放回地从一个总数n的样本中抽取m个样本（batch size），进行n/m次迭代后称为一次epoch（即我们一共用了整个数据集大小数量的样本进行了训练）。
在自助法的采样过程中，对n个样本进行n次自助抽样，当n趋于无穷大时，最终有多少数据从未被选择过？
$$
\lim_{n \rightarrow \infty}(1-\frac{1}{n})^{n}=\lim_{n \rightarrow \infty} \frac{1}{(1+\frac{1}{n-1})^{n}}=\frac{1}{\mathrm{e}} \approx 0.368
$$
# 判别模型和生成模型
生成模型：学习得到联合概率分布P（x,y），即特征x和标记y共同出现的概率，然后求条件概率分布。能够学习到数据生成的机制。如贝叶斯分类器。
判别模型：学习得到条件概率分布P（y|x），也就是特征x出现的情况下标记y的出现概率。
生成模型需要的数据量比较大，能够较好地估计概率密度；而判别模型对数据样本量的要求没有那么多。由生成模型可以得到判别模型，但由判别模型得不到生成模型。
# L1正则化/L2正则化
适定问题指定解满足下面三个要求的问题:解是存在的；解是唯一的；解连续依赖于定解条件，即解是稳定的。这三个要求中，只要有一个不满足，则称之为不适定问题。
求解不适定问题的普遍方法是用一组与原不适定问题相“邻近”的适定问题的解去逼近原问题的解,这种方法称为正则化方法。
正则化就是对最小化经验误差函数上加约束，这样的约束可以解释为先验知识（正则化参数等价于对参数引入先验分布）。约束有引导作用，在优化误差函数的时候倾向于选择满足约束的梯度减少的方向，使最终的解倾向于符合先验知识。
L1正则化项即所有权重的绝对值之和。L2正则化项即所有权重的平方之和。当我们在损失函数中加入正则化项时还要乘以一个λ。
以L2正则化项为例，损失函数增加了L2项后，梯度下降时求导如下：
$$
L^{\prime}(\theta)=L(\theta)+\lambda \frac{1}{2}(||\theta||_{2})
$$
$$
\frac{\partial \mathrm{L}^{\prime}}{\partial w}=\frac{\partial \mathrm{L}}{\partial w}+\lambda w
$$
则每次更新参数w的公式为:
$$
W^{t+1} \leftarrow W^{t}-\eta \frac{\partial \mathrm{L}^{\prime}}{\partial W}=w^{t}-\eta(\frac{\partial \mathrm{L}}{\partial w}+\lambda w^{t})
$$
$$
=(1-\eta \lambda) w^{t}-\eta \frac{\partial \mathrm{L}}{\partial w}
$$
可以发现与没有正则化项的损失函数相比，每次梯度下降更新参数w时多减了一项:
$$
\eta \lambda  w^{t}
$$
从某个角度来说带有正则化项的损失函数也可以看成是拉格朗日函数。
# L1/L2 范数的相同点和不同点
**相同点:**
限制模型的学习能力——通过限制参数的规模，使模型偏好于权值较小的目标函数，防止过拟合。
**不同点:**
L1 正则化可以产生更稀疏的权值矩阵，可以用于特征选择，同时一定程度上防止过拟合；L2正则化主要用于防止模型过拟合。
L1 正则化适用于特征之间有关联的情况；L2 正则化适用于特征之间没有关联的情况。
# 深度学习训练、调参Tips
假如深度学习得到的模型表现性能不好，该如何调参呢？

首先检查模型在训练集上的表现，如果在训练集上性能较好，而在测试集上性能不好，那么这种情况就叫做过拟合。我们可以在训练过程中每隔固定steps数运行测试检查验证集预测准确率来及早发现过拟合的发生，这又叫早停。
层数较深的网络需要更多的训练迭代次数才能得到较好的性能，在同样的迭代次数上比较两个层数不同的模型的性能时，有时会发现一个层数少的模型的损失比层数多的模型还要下降的快，但这种情况有可能只是层数深的模型还没有训练好，需要更多训练次数。层数深的模型也不一定就比层数浅的模型最终表现要好。
在训练时使用dropout层可以有效地减少过拟合，但是会延长训练过程，因为dropout后每轮更新的参数数量变少了（但更新的参数的梯度变大了，更新的更快），需要更多轮训练迭代才能将所有参数更新到性能较好的水平。而在测试时，我们应当关闭dropout层，使所有神经元都参与运算。
使用残差网络也可以有效地减少过拟合，还可以大大减少计算量，加快训练速度。
使用批标准化也可以有效减少过拟合，还可以加快训练速度。
梯度消失现象是由于反向传播的梯度是链式法则求导，如果激活函数使用类似sigmoid的激活函数，求导函数的特性会使得最初的几层的梯度变得很小，权重参数更新很慢。这也是激活函数从sigmoid函数改动relu函数的原因。
测试时出现过拟合情况，除了使用dropout之外，我们还可以使用正则化项。
训练过程中为了避免出现固定学习率在训练后期过大而导致在局部最优解两边反复震荡的情况，我们可以使用具有自适应学习率的优化算法，比如Adagrad。
在训练过程中，每隔一定的迭代轮次，使用验证集来测试模型准确率，这个可以及早发现模型是否出现过拟合。
梯度爆炸即不同层之间的梯度（值大于1.0）相乘，使得权重更新时呈指数级爆炸增长。在极端情况下，权重的值变得非常大，以至于溢出，导致NaN值。我们可以在训练过程中检查和限制梯度的大小，这就是梯度截断。我们还可以使用LSTM单元来减少梯度爆炸问题。此外，增加正则化项也可减少梯度爆炸现象的产生。使用批标准化也可减轻梯度爆炸现象。
# 梯度消失/梯度爆炸的概念
如神经网络使用sigmoid作为激活函数，这个函数有个特点，就是能将负无穷到正无穷的数映射到0和1之间，并且对这个函数求导的结果是
$$
f^{\prime}(x)=f(x)(1-f(x))
$$
因此两个0到1之间的数相乘，得到的结果就会变得很小。当神经网络层数非常深的时候，最后一层产生的梯度就因为乘了很多的小于1的导数而越来越小，最终就会变为0，从而导致层数比较浅的权重没有更新，这就是梯度消失。
初始化权值w时过大，导致前面层会比后面层变化的更快，这样就会导致权值越来越大，这就是梯度爆炸。即网络层之间的梯度（值大于1.0）重复相乘导致的指数级增长会产生梯度爆炸。在深层网络或循环神经网络中，误差梯度可在更新中累积，变成非常大的梯度，然后导致网络权重的大幅更新，并因此使网络变得不稳定。在极端情况下，权重的值变得非常大，以至于溢出，导致 NaN值。
# 特征标准化
特征标准化（Feature Scaling），就是让不同的特征值具有相同的缩放程度。
**z-score标准化:**
$$
y_{i}=\frac{x_{i}-\overline{x}}{s}
$$
$$
\overline{x}=\frac{1}{n} \sum_{i=1}^{n} x_{i}, \quad s=\sqrt{\frac{1}{n-1} \sum_{i=1}^{n}\left(x_{i}-\overline{x}\right)^{2}}
$$
**min-max标准化:**
$$
X_{n o m}=\frac{X-X_{\min }}{X_{\max }-X_{\min }}
$$
# 什么是批标准化（Batch Normalization）
论文:Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift
论文地址:https://arxiv.org/pdf/1502.03167.pdf 。
深层神经网络在每一层做线性变换和激活后，得到的值随着网络深度加深或在训练过程中，其分布会逐渐发生偏移或者变动，之所以训练收敛慢，一般是整体分布逐渐往非线性函数（如sigmoid函数）的取值区间的上下限两端靠近，这会导致反向传播时最后几层神经网络的梯度越来越小，出现梯度消失的现象。
批标准化就是对每层神经元的原始激活值通过标准化规整成服从正态分布的值，把线性变换后得到的值的分布强行拉回到均值为0方差为1的标准正态分布，这样使得这些值落在非线性函数中对输入比较敏感的区域，这样输入值的小变化就会导致损失函数较大的变化，梯度就会变大，避免梯度消失的问题。而且梯度变大意味着学习收敛速度快，能大大加快训练速度。
我们要注意一点的是，对数据进行批标准化一般都将其分布拉回标准正态分布上，但有时训练集数据并不服从正态分布，这同样会导致网络的性能下降。为解决该问题，我们引入两个新的参数γi,βi。γi和βi是在训练时网络自己学习得到的，它们用来恢复要学习的数据集特征。 
**批标准化计算过程:**
首先将原始激活值规整到均值为0，方差为1的正态分布范围内。其中，ai为某个神经元原始激活值,μ和σi分别为期望和标准差，要计算期望和标准差，我们必须确定一个数据范围。
$$
\tau=\frac{a_{i}-\mu}{\sigma_{i}}
$$
接下来根据我们采用的神经网络分两种情况:
* 如果是全连接神经网络，对于我们最常见的mini-batch形式的训练，假如某个mini-batch有n个样本，每个样本在某层的某个神经元k上都会产生一个原始激活值，mini-batch中n个样本在通过这个神经元k时产生n个激活值，那么我们就用这n个激活值来计算对这个神经元k的激活值的期望和方差；
* 如果是CNN神经网络，对于我们最常见的mini-batch形式的训练，假如某个mini-batch有n个样本，对于某层中的单个卷积核，n个样本通过这个卷积核时产生了N个特征图，假设每个特征图长X宽=RXL，那么对于这个卷积核上的所有特征值，它们的期望和标准差就由这RXLXN个值产生。
  然后我们要在模型训练过程中随其他权重参数一起，学习两个调节因子γi和βi，对上面规整后的值进行微调。因为经过第一步操作后，批标准化有可能降低神经网络的非线性表达能力，所以会以此方式来补偿批标准化操作后的神经网络的表达能力。
  之后，我们要学习γi和βi两个调节因子，用来恢复要学习的数据集特征。γi和βi的学习是自动的，同网络中其他权重参数一样，是通过反向传播在训练过程中自动学习的。 下式中，ai_norm为经过标准化操作后的值。
$$
a_{i}^{norm}=\gamma_{i} \cdot \tau+\beta_{i}
$$

# 为什么交叉熵损失可以提高具有 sigmoid 和 softmax 输出的模型的性能，而使用均方误差损失则会存在很多问题
因为sigmoid函数求导后是
$$
f^{\prime}(x)=f(x)(1-f(x))
$$
如果损失函数是平方误差函数，那么求导后的式子易导致更新梯度变得很小。
而使用交叉熵函数作损失函数后，与softmax函数反向传播求导时的式子就很简单，而且不会造成梯度消失。
# softmax函数在框架中计算时的优化
在机器学习中，使用softmax函数时输入数据往往是神经网络最后一层的输出值，通过softmax函数可以转化成所有神经元输出的值的和为1，因此可以看成概率来理解，这样我们就可以用来进行多分类。
实际计算时为了防止上溢，将e的x次方的分子分母上的x都减去一个xmax。防止下溢可在分母中加上一个小的常数即可。
# 激活函数的作用
如果没有激活函数，那么无论多少层网络，最终得到的函数都可以表示成一个线性组合。只有加入激活函数后，才能得到复杂的多层非线性模型，拟合能力更加强大。
神经网络具有至少一个非线性隐藏层，那么只要给予网络足够数量的隐藏单元，它就可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的函数。如果不使用非线性激活函数，那么每一层输出都是上层输入的**线性组合。
# 反向传播的过程
梯度下降法在每轮迭代时计算损失函数对所有权重参数的梯度（偏导数），然后用梯度更新权重参数，然后下一轮迭代时重新计算损失函数值。比较损失函数增大还是缩小了，将权重参数朝着损失函数值缩小的方向更新。
# 卷积神经网络原理概述
卷积神经网络（Convolutional Neural Networks）由许多个卷积层-池化层-dropout层这样的三层小结构串联而成（注意池化和dropout层都不一定是必须的），在网络的末尾往往还要接上1-2层全连接层神经网络，最后接输出层。
卷积层中最重要的概念就是卷积计算，一个卷积层中有多个卷积核（比如32、64、128）,同一个卷积层中的卷积核尺寸都相同，卷积核依照卷积层定义中的步长（行上和列上都有步长），从一个输入的图片样本数据矩阵的左上角开始依次扫过，每次都从图片样本数据矩阵中提取一个和卷积核大小相同的矩阵，称之为感受域。卷积核与这个矩阵做矩阵同样位置上元素相乘的乘积再求和的计算，最后再与卷积层中的bias变量相加，就得到了feature map中的一个位置上的特征值。一个卷积核将一个图片样本数据完整地扫完后，就得到一个完整的feature map。一个卷积层中有多少个卷积核，本层卷积结束后就能得到多少层feature map。
池化层的重要作用是浓缩feature map，减少feature map的数据量，同时又保留feature map中最有表现力的特征值，这样还可以减轻过拟合。以最常见的最大池化为例，filter窗口往往设为2x2，步长一般和窗口边长一样也为2，那么最大池化就是在2x2的窗口中选取一个最大值保留下来。窗口按照步长将整个feature map扫完，最后得到一个长宽均缩小为原来一半的新feature map。
dropout层按照我们设定的比率随机地保留feature map中这个比例数的特征值，而将其他特征值置为0。同时被保留的特征值都要除以这个比率，相当于把这部分特征值都放大了。dropout层可以有效地减轻过拟合，但因为反向传播时特征值为0的神经元权重都不更新，因此也延长了训练时间。
全连接层用来进一步把特征图压缩（压缩前会把特征图拉平为1维向量），多个全连接层最终将特征向量中的元素数量压缩到刚好等于我们需要预测的类别数。最后使用softmax函数将特征向量归一化就可以得到预测结果。
CNN卷积得到的特征图具有空间不变性，特征图上不同位置的特征值与输入样本特征分布一致。CNN网络中卷积层的权值是共享的，卷积层中的所有卷积核中的权重对一个图片样本的矩阵上的所有元素来说是共享的。
# 卷积神经网络feature map尺寸计算
在每个卷积层中，有多少个卷积核，就有多少层特征图（或者说输出通道数）。对于输入的图片数据，如果是灰度图片，那么输入数据通道数为1；如果是彩色图片，那么输入通道数为3。
卷积后得到feature map边长计算公式： 
$$
h_{out} =\left(\left(h_{in}+p^{*} 2-k\right) / s\right)+1
$$
h（in）为输入图片的长或宽，p为边缘要填充的行或列数，k为卷积核尺寸，s为步长，这里假设的是图片、卷积核、步长在长和宽方向均相等的情况。如果长和宽不等，仍然按上式计算，只要记得是用同样的对应位置上的数据来计算即可。
卷积层之后进行pooling后feature maps边长计算公式：
$$
pool1_{h}=((conv1_{h}-k_{p})/s_{p})+1
$$
其中conv1（h）为卷积层输出的特征图尺寸，kp为池化层窗口边长，sp为池化层步长。
**举例：**
假设输入图片尺寸为32X32，使用6个5X5的卷积核，步长为1，pading=0。那么这一层卷积后的feature map尺寸为：
$$
(32-5) / 1+1=28
$$
即卷积层尺寸为28X28。
卷积层后假如pooling层的filter=2X2，步长stride=1，pooling方法为最大池化。那么pooling层后的feature map的边长为：
$$
(28-2) / 2+1=14
$$
即pooling后尺寸为14X14。
# RNN神经网络原理概述
RNN（Recurrent Neural Network）的产生，就是为了让神经网络对序列性输入拥有处理能力。比如语音、文本等。RNN之所以会被叫做循环，是因为RNN对输入的序列中的每个时刻的样本都执行相同的计算操作。
RNN是一种拥有记忆能力的学习器，它对到目前为止所计算过的序列拥有一定的记忆能力。RNN的核心内容就是使用了循环连接，使用循环链接之后，可以使得神经网络拥有动态记忆能力。所谓循环连接，指的就是上一时刻的隐藏单元的输出s（t）也作为下一时刻的隐藏单元的输入，参与下一个时刻对s（t+1）的计算。
RNN能够处理不定长的输入数据，但是在某个时刻的输入还是定长的。RNN的权值同样是共享的，即RNN神经元中的权重对于不同时刻的U、W、V都是共享的。
某一个时刻的RNN神经元包含输入层、隐藏层、输出层。它们各自有一个权重矩阵，分别为U、W、V。在本时刻输入样本数据为x（t），与x（t）相乘的权重为U，上一时刻的隐藏层为s（t-1），与s（t-1）相乘的权重为W，输出为o（t），o（t）由本层记忆与权重矩阵V相乘得到。
如果对RNN网络的架构仍不是很清楚，请在这个网站上查看：http://www.asimovinstitute.org/neural-network-zoo/ 。
**RNN神经网络的计算过程:**
对某一个时刻的单个RNN神经元来说，t时刻的输入向量x（t）与输入层的权重矩阵U相乘，上一时刻的隐藏层输出（即我们所说的记忆）s（t-1）与本时刻的隐藏层权重矩阵W相乘，两者相加再经过激活函数（如sigmoid函数）得到的输出值就是本时刻隐藏层的输出值s（t）。s（t）再与本时刻输出层权重矩阵V相乘，得到的结果再经过激活函数（通常是softmax函数）得到的输出值就是本时刻的输出值o（t）。
每一个时刻t都会保存两个值：隐藏层的输出值（即记忆）s（t）和输出层输出值o（t）。其中s（t）会参与下一个时刻的计算。
需要注意的是，每一步的输出并不是必须的。假如我们把图片的每一行像素作为每个时刻的输入，最后我们要预测图片输入哪个类，那么我们只关心最后一个时刻的输出。
用公式来表示计算过程如下：
$$
s_{t}=f_{1}\left(U x_{t}+W_{S_{t-1}}\right)
$$
$$
o_{t}=f_{2}\left(V_{S_{t}}\right)
$$
**RNN的训练过程：**
对RNN的训练，也使用反向传播（backpropagation）算法，但反向传播链式求导计算时基于的是时刻（RNN中的不同时刻就类似CNN中的不同层），被称为BPTT算法。
但要注意一点的是，由于RNN的参数被所有时刻共享，因此当前时刻的输出不仅要依靠当前时刻所计算出来的梯度，还依赖于之前所有时刻上计算得到的梯度。即下面的计算公式：
$$
\frac{\partial E}{\partial W}=\sum^{t} \frac{\partial E_{t}}{\partial W}=\sum_{k=0}^{t} \frac{\partial E_{t}}{\partial n e t_{t}} \frac{\partial n e t_{t}}{\partial s_{t}}\left(\prod_{j=k+1}^{t} \frac{\partial s_{t}}{\partial s_{k}}\right) \frac{\partial s_{k}}{\partial W}
$$
如时刻t=4时，还需要向后传递三步，已经后面的三步都需要加上各种梯度。
关于BPTT算法，可看这篇文章：On the difficulty of training recurrent neural networks，文章地址: http://proceedings.mlr.press/v28/pascanu13.pdf 。
基于BPTT来训练RNN，由于其训练存在递归的问题，如果RNN的递归深度过长（一般超过十个时刻就难以为继），将会带来所谓的梯度消失或梯度爆炸问题。
# LSTM神经网络原理概述
LSTM（Long Short Memory Network）即长短时记忆网络。它是为了克服RNN无法很好地处理长期记忆而提出的。由于RNN的隐藏层输出（即所谓的长期记忆）s（t）中每一个时刻都要经过激活函数sigmoid处理，而sigmoid函数求导后形式为sigmoid（1-sigmoid），这会导致对于靠前的时刻梯度快速减小，也就是梯度消失现象。为了避免梯度消失，人们对RNN网络进行了改进，实现了一种较为复杂的累加形式的反向传播，这就是LSTM网络。
对于某一个时刻的LSTM网络单个神经元来说，它包含三个结构：遗忘门、输入门和输出门。遗忘门负责决定保留多少上一时刻的单元记忆到当前时刻的单元记忆；输入门负责决定保留多少当前时刻的输入到当前时刻的单元记忆；输出门负责决定当前时刻的单元记忆有多少输出。
**LSTM网络计算过程:**
对于某一个时刻的LSTM网络单个神经元：
每个时刻的LSTM单个神经元有三个输入，即上一时刻的长时单元记忆C（t-1）、上一时刻单元的输出h（t−1）和当前时刻的输入x（t）。输出有两个值，C（t）代表了长时记忆，h（t）则代表了短时记忆。
遗忘门是上一时刻输出h（t−1）和本单元输入x（t）做矩阵乘法，再左乘遗忘门权重矩阵W（f），最后加上遗忘门的偏置b（f），将得到的值输入sigmoid函数后得到一个[0,1]内的激活值，来控制上一单元状态被遗忘的程度。
其中中括号表示两个向量相连合并。设输入层维度为dx，隐藏层维度为dh，上面的状态维度为dc，则W（f）的维度为dc（dh+dx）。
$$
f_{t}=\sigma(W_{f}\cdot [h_{t-1}, x_{t}]+b_{f})
$$
输入门的计算分为两部分：i（t）的计算与遗忘门的计算方法一致，只是权重全部换成了输入门i中的权重；头顶带波浪的C（t）的计算与遗忘门的计算也基本一致，权重全部换成了输入门C中的权重，但激活函数变为tanh函数，产生一个[-1,1]之间的值。
$$
i_{t}=\sigma(W_{i}\cdot [h_{t-1}, x_{t}]+b_{i})
$$
$$
\tilde C_{t}=\tanh (W_{C}\cdot [h_{t-1}, x_{t}]+b_{C})
$$
下面就是更新长期记忆C（t）的计算，当前时刻的单元长期记忆C（t）由遗忘门输入和上一时刻状态的积加上输入门两部分的积组成。
$$
C_{t}=f_{t}\cdot  C_{t-1}+i_{t}\cdot \tilde C_{t}
$$
输出门的计算方式和遗忘门仍相同，只是权重全部换成了输出门O中的权重，计算出的o（t）用来控制当前时刻的单元记忆有多少输出。最后本时刻的短期记忆h（t）即为o（t）与tanh（C（t））的乘积。
$$
o_{t}=\sigma(W_{o}\cdot [h_{t-1}, x_{t}]+b_{o})
$$
$$
h_{t}=o_{t} \cdot \tanh (C_{t})
$$
# GRU神经网络原理概述
GRU神经网络（Gated Recurrent Unit，门控循环单元神经网络）是LSTM神经网络的一个变体。GRU神经网络只有两个门，分别是更新门和重置门。GRU神经网络实际上是将LSTM神经网络中的遗忘门和输入门合成了一个单一的更新门。
重置门用于控制忽略前一时刻的记忆的程度，重置门的值越小说明忽略得越多。更新门用于控制前一时刻的记忆被带入到当前状态中的程度，更新门的值越大说明前一时刻的记忆带入越多。
**具体计算过程如下:**
rt是重置门，zt是更新门，ht-1是上一时刻传来的长时记忆，xt是本时刻输入的信息。rt和zt首先产生两个0到1之间的阈值。
$$
r_{t}=\sigma\left(W_{r} \cdot\left[h_{t-1}, x_{t}\right]\right)
$$
$$
z_{t}=\sigma\left(W_{z} \cdot\left[h_{t-1}, x_{t}\right]\right)
$$
然后计算头顶带波浪号的ht（候选隐含记忆），计算时rt这个重置门阈值控制了ht-1带入到带波浪ht（候选隐含记忆）中的信息量，重置门值越小则上一时刻的长时记忆ht-1的信息被丢弃的越多。然后用带波浪ht和ht-1更新本时刻的长时记忆ht。zt就是更新门，更新门可以控制过去的长时记忆ht-1在当前时刻的重要性。如果更新门一直近似1，过去的隐含状态将一直通过时间保存并传递至当前时刻。
$$
\tilde h_{t}=tanh((W_{\tilde{h}}) \cdot[r_{t} \cdot h_{t-1}, x_{t}])
$$
$$
h_{t}=(1-z_{t}) \cdot h_{t-1}+z_{t} \cdot (\tilde h_{t})
$$
其中[]表示两个向量相连接，点号表示矩阵元素相乘。