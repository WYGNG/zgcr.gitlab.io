---
title: 深度学习中的优化算法介绍
date: 2019-03-06 14:47:59
tags:
- 深度学习
categories:
- 深度学习
mathjax: true
---

# BGD/MBGD/SGD
若有:
$$
\theta^{<0>}=\left[ \begin{array}{l}{\theta_{1}^{<0>}} \\ {\theta_{2}^{<0>}}\end{array}\right]
$$
那么
$$
\nabla L(\theta)=\left[ \begin{array}{l}{\partial L\left(\theta_{1}\right) / \partial \theta_{1}} \\ {\partial L\left(\theta_{2}\right) / \partial \theta_{2}}\end{array}\right]
$$
梯度下降的公式为:
$$
\theta^{<i+1>}=\theta^{<i>}-\eta \nabla L\left(\theta^{<i>}\right)
$$
即对某个参数w的梯度下降公式就是上一步的该参数w的值减去学习率乘以损失函数对w的偏导数值(代入上一步中所有输入x的值)。损失函数对w的偏导数值就是梯度。需要注意的是梯度下降总是减去梯度。
BGD/MBGD/SGD的区别主要在于损失函数的计算：
$$
L=\sum_{j}\left(\hat{y}^{(j)}-\left(b+\sum w_{i} x_{i}^{(j)}\right)\right)^{2}
$$
BGD中，用于计算损失函数的是全体样本(epoch)；MBGD中，用于计算损失函数的是一个小批量样本(batch size)；SGD中，用于计算损失函数的是单个样本。
# Momentum
$$
\begin{aligned} v_{t} &=\gamma v_{t-1}+\eta \nabla_{\theta} J(\theta) \\ \theta &=\theta-v_{t} \end{aligned}
$$
θ为权重参数，衰减值γ通常设定为0.9，或相近的某个值。
可以看到参数更新时不是直接减去学习率乘以上一步梯度，而是减去动量vt。vt由上一步的动量vt-1乘以衰减值γ，再加上上一步的学习率乘以梯度。
这样做的好处是，如果当前步的梯度与上一步梯度方向相同，那么这种计算会增大动量vt；如果当前步的梯度与上一步梯度方向相反，则会减小动量vt。
这样如果我们再局部最优点附件震荡收敛时，当前步的梯度与震荡的前一步的梯度方向相反。因此原本在局部最优点要大幅震荡徘徊的梯度，主要受到前一时刻的影响，而导致在当前时刻的梯度幅度减小。 这使得Momentum算法收敛速度比普通的梯度下降法更快。
# NAG(Nesterov accelerated gradient)
$$
\begin{array}{l}{v_{t}=\gamma v_{t-1}+\eta \nabla_{\theta} J\left(\theta-\gamma v_{t-1}\right)} \\ {\theta=\theta-v_{t}}\end{array}
$$
θ为权重参数，衰减值γ通常设定为0.9。
NAG是对Momentum的改进。计算vt时求梯度时，损失函数中的参数θ会减去上一步的动量vt-1与衰减值γ的乘积，然后再对这个损失函数求梯度。
这使得我们能预测参数θ下一位置的近似值。这样我们就可以通过计算参数未来位置的近似值上的梯度"预见未来"。
NAG可以使RNN神经网络在很多任务上有更好的表现。
# Adagrad
$$
\eta^{<t>}=\frac{\eta}{\sqrt{t+1}} \quad g^{<t>}=\frac{\partial L\left(\theta^{<t>}\right)}{\partial \theta}\\
\sigma^{<t>}=\sqrt{\frac{1}{t+1} \sum_{i=0}^{t}\left(g^{<i>}\right)^{2}}\\ \theta^{<t+1>} \leftarrow \theta^{<t>}-\frac{\eta^{<t>}}{\sigma<t>} g^{<t>}
$$
 化简，得:
$$
\theta^{<t+1>} \leftarrow \theta^{<t>}-\frac{\eta}{\sqrt{\sum_{i=0}^{t}(g<i>)^{2}}} g^{<t>}
$$


 θ为权重参数。t代表每一次迭代。g(t)代表第t次迭代时对θ得梯度。ϵ为平滑因子，避免除数为零。
 可以看到学习率是初始学习率η除以过去每次迭代的梯度的平方之和。所以学习率会逐渐变小。对于变化不剧烈的参数，它们的梯度一直比较小，因此学习率就可以保持在比较大的水平；而对于变化剧烈的参数，它们的梯度值比较大，学习率相对就小一些。
基于上面这个特性，Adagrad方法对稀疏参数进行大幅更新和对频繁参数进行小幅更新。因此，Adagrad方法非常适合处理稀疏数据。
# AdaDelta/RMSProp
论文:ADADELTA: an adaptive learning rate method
论文地址:https://arxiv.org/pdf/1212.5701v1.pdf 。
$$
E\left[g^{2}\right]_{t}=\rho E\left[g^{2}\right]_{t-1}+(1-\rho) g_{t}^{2}
\\ RMS[g]_{t}=\sqrt{E\left[g^{2}\right]_{t}+\epsilon} \\
\Delta x=-\frac{R M S[\Delta x]_{t-1}}{R M S[g]_{t}} \cdot g_{t}\\
E\left[\Delta x^{2}\right]_{t}=\rho E\left[\Delta x^{2}\right]_{t-1}+(1-\rho) \Delta x^{2}\\
x_{t+1}=x_{t}+\Delta x_{t}
$$
论文中，作者给出的两个超参数的合适实验值:ρ=0.95，ϵ=1e−6。
AdaDelta是对Adagrad方法的改进。AdaDelta基本思想是用一阶的方法，近似模拟二阶牛顿法。Adagrad会累加之前所有的梯度平方，而Adadelta只累加固定大小w步内的过往梯度，并且也不直接存储这些项，仅仅是近似计算其对应的平均值。
从多个数据集情况来看，AdaDelta在训练初期和中期，具有非常不错的加速效果。但是到训练后期，进入局部最小值雷区之后，AdaDelta就会反复在局部最小值附近抖动。主要体现在验证集错误率上，脱离不了局部最小值吸引盆。这时，切换成动量SGD，如果把学习率降低一个量级，就会发现验证集正确率有2%~5%的提升。
个人猜测使用SGD时，因为人工学习率的量级降低，给训练造成一个巨大的抖动，从一个局部最小值，抖动到了另一个局部最小值，而AdaDelta的二阶近似计算，或者说所有二阶方法，则不会产生这么大的抖动，所以很难从某个局部最小值中抖出来。
RMSProp和AdaDelta基本一样。
# Adam(adaptive moment estimation)
论文:ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION
论文地址:https://arxiv.org/pdf/1412.6980.pdf 。
$$
g_{t}=\nabla_{\theta} J\left(\theta_{\mathrm{t}-1}\right)\\
m_{t}=\beta_{1} m_{t-1}+\left(1-\beta_{1}\right) g_{t}\\
v_{t}=\beta_{2} v_{t-1}+\left(1-\beta_{2}\right) g_{t}^{2}\\
\hat{m}_{t}=m_{t} /\left(1-\beta_{1}^{t}\right)\\
\hat{v}_{t}=v_{t} /\left(1-\beta_{2}^{t}\right)\\
\theta_{\mathrm{t}}=\theta_{t-1}-\alpha * \hat{m}_{t} /\left(\sqrt{\hat{v}_{t}}+\varepsilon\right)
$$
Adam算法吸收了RMSProp和Momentum算法的优点。其计算过程如下:
* 计算第t次迭代的梯度；
* 计算梯度的指数移动平均数，m0初始化为0。类似于Momentum算法，综合考虑之前时间步的梯度动量。β1系数为指数衰减率，控制权重分配(动量与当前梯度)，默认为0.9；
* 其次，计算梯度平方的指数移动平均数，v0初始化为0。β2系数为指数衰减率，控制前面的的梯度平方的影响情况，默认为0.999；
* 由于m0初始化为0，会导致mt偏向于0，尤其在训练初期阶段。因此需要对梯度均值mt进行偏差纠正，降低偏差对训练初期的影响；
* v0初始化为0导致训练初始阶段vt偏向0，对其进行纠正；
* 更新权重参数θ，初始的学习率α乘以梯度均值与梯度方差的平方根之比。其中默认学习率α=0.001，ε=10^-8，避免除数变为0。