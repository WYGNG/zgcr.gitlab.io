---
title: 深度学习中的激活函数介绍
date: 2019-03-05 18:51:15
tags:
- 深度学习
categories:
- 深度学习
mathjax: true
---

# Sigmoid函数
$$
\sigma(x)=\frac{1}{1+e^{-x}}
$$
值域在[0,1]之间，图像y轴对称。图像中输入只要离开坐标原点一定距离，函数的导数就很小了，因此sigmoid函数容易引起梯度消失。
# tanh函数
$$
\tanh (x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}
$$
值域[-1,1]。与sigmoid函数类似，图像中输入只要离开坐标原点一定距离，函数的导数就很小了，因此tanh函数也容易引起梯度消失。
# relu函数
$$
\operatorname{ReL} U(x)=\left\{\begin{array}{ll}{x} & {\text { if } x>0} \\ {0} & {\text { if } x \leq 0}\end{array}\right.
$$
输入<0，输出都是0；输入>0，输出等于输入。在输入为正数的时候，函数导数为1，因此不存在梯度消失问题。作为激活函数时网络模型的收敛速度远快于sigmoid和tanh。
# Maxout函数
假如一个简单的神经网络输入层有x1和x2两个神经元，下一层有4个神经元，x1和x2分别与4个神经元做运算，得到4个输出值，如果我们使用relu函数，那么这4个输出值应当都要经过relu函数计算得到4个激活值。而使用Maxout则是将这个4个值分成两组(即一个组里2个神经元)，分别取每组的最大值，最后只输出两个最大值。
Maxout学习出来的激活函数是分段的线性函数。当分组中神经元数量越多时，分段函数的分段就越多，拟合能力越强。事实上relu就可以看成是Maxout函数的一种特殊情况。
Maxout函数在每次反向传播时总是将分组中其他较小的值当做不存在，只更新最大值所在的神经元权重参数。这也与relu类似，relu每次反向传播时对神经元值为0的神经元权重参数不更新(因为梯度被计算为0)。
# Softplus函数
$$
Softplus(x)=\log \left(1+e^{x}\right)
$$
Softplus函数可以看成是ReLU函数的平滑版本。Softplus函数是对全部数据进行了非线性映射，是一种不饱和的非线性函数。它的收敛速度比ReLU函数要慢很多。但是计算量比relu函数要大，因此函数的导数计算起来较为复杂。
# softmax函数
$$
\sigma(z)_{j}=\frac{e^{z_{j}}}{\sum_{k=1}^{K} e^{z_{k}}}
$$
softmax函数压缩一维向量中的每个元素在0到1之间，并且所有元素总和为1。softmax函数最好在分类器的输出层使用，用来预测输入物体属于某个类的概率。