---
title: 目标检测模型性能指标mAP的含义解释
date: 2019-05-05 22:28:18
tags:
- 目标检测
categories:
- 目标检测
mathjax: true
---

# 准确率、精确率、召回率、P-R曲线图、平衡点、F1 score、Fβ
```
真实情况    预测为正        预测为反	
正	       TP（真正例）	 FN（假反例）	
反	       FP（假正例）	 TN（真反例）	
```
## 准确率
$$
Accuracy=\frac{TP+TN}{\text {AllSamples}}
$$
## 精确率
 对正类的精确率公式:
$$
P=\frac{T P}{T P+F P}
$$
即预测为真的样本中有多少实际为真。
## 召回率
对正类的召回率公式:
$$
R=\frac{T P}{T P+F N}
$$
即真实为真的样本中有多少被预测为真。
## P-R曲线图
一般来说，我们希望精确率和召回率同时越高越好，然而精确率和召回率是一对矛盾的度量，一个高时另一个就会偏低。我们可以做出P-R曲线图来表示精确率和召回率的关系。
如果一个学习器的P-R曲线被另一个学习器的P-R曲线包围，则可以断言后面的学习器要好些；如果两个曲线有交叉，一个比较合理的判据是比较两个曲线与x轴和y轴围成的面积的大小，但这个面积不好计算。
## 平衡点
上面说了曲线与x轴和y轴围城的面积不好计算，因此我们就找一个准确率=召回率的值，这个值称为平衡点。
## F1 score
F1是准确率和召回率的调和平均数，即
$$
\frac{1}{F 1}=\frac{1}{2} \times\left(\frac{1}{P}+\frac{1}{R}\right)
$$
可写为
$$
F 1=\frac{2 P R}{P+R}
$$
如果我们对P和R的权重不同，可以将上式稍作变形，得到Fβ公式:
## Fβ
Fβ是准确率和召回率的加权调和平均数，即:
$$
\frac{1}{F_{\beta}}=\frac{1}{1+\beta^{2}} \times\left(\frac{1}{P}+\frac{\beta^{2}}{R}\right)
$$
$$
F_{\beta}=\frac{\left(1+\beta^{2}\right) P R}{\beta^{2} P+R}
$$
# VOC 2007/2010中AP和mAP的计算方式
首先使用目标检测模型计算检出的所有样本对某一个类的置信度（confidence）和回归框坐标。然后，我们计算每一个检测样本的回归框坐标与样本的真值框（ground truth）的IOU，如果大于0.5，则置该样本的ground truth label为1，否则为0；
然后我们将所有检测到的样本按照置信度从大到小排序；
接下来的计算方式分两种:
1. 对于VOC 2007数据集，设定一组recall阈值，[0, 0.1, 0.2, …, 1]。然后对于每个recall阈值，都对应着很多种top取法（比如TOP1，TOP5），每个recall值对应的多种top取法中（包括等于此recall的取法）有一个最大的precision。这样，我们就计算出了11个最大precision。AP即为这11个最大precision的平均值。这种方法英文叫做11-point interpolated average precision。
2. 对于VOC 2010以后的数据集，采用了一种新的计算方法。假设检测到的N个样本中有M个样本的ground truth label为1（即正例），那么我们会得到M个recall值（1/M, 2/M, ..., M/M）。对于每个recall值，都对应着很多种top取法（比如TOP1，TOP5），每个recall值对应的多种top取法中（包括等于此recall的取法）有一个最大的precision，把每种recall对应最大的precision求和取平均值即为AP的值。

AP衡量的是学出来的模型在每个类别上的好坏，mAP衡量的是学出的模型在所有类别上的好坏。在得到每个类别上的AP后，我们只需要取所有AP的平均值就得到mAP的值。

# VOC 2010中AP计算实例
该举例使用的是VOC 2010以后的数据集的计算方法。
假如共有10个类，现在有20个样本，现在判断每个样本为其中一类car的置信度（confidence）如下表所示，降序排列。ground_truth_label=1即预测样本的回归框与真实框的IOU大于0.5，ground_truth_label=0则预测样本的回归框与真实框的IOU小于等于0.5。
```
id    confidence    ground_truth_label
4        0.91        1
2        0.76        1
13       0.65        0
19       0.46        0
6        0.45        0
9        0.38        1
16       0.24        1
1        0.23        0
18       0.23        0
5        0.13        0
7        0.12        1
15       0.12        0
10       0.11        0
17       0.1         0
12       0.09        0
20       0.08        1
14       0.07        0
8        0.03        0
11       0.03        0
3        0.01        0
```
**从表中ground_truth_label可以看出有6个正例，14个负例。VOC 2010及之后的数据集计算AP的方式是：假设N个样本中有M个正例，如上表我们N是20，M是6，则有6种recall值，分别为1/6，2/6，3/6，4/6，5/6，6/6。对于每个recall值，都对应着多种top的取法对应的precision和recall。每个recall值对应的多种top取法中（包括等于此recall的取法）有一个最大的precision，把每种recall对应最大的precision求和取平均值即为AP的值。**

**我们先以top5取法为例:**
```
id    confidence    ground_truth_label
4        0.91        1
2        0.76        1
13       0.65        0
19       0.46        0
6        0.45        0
```
top5样本中id为4和2的样本被判定为正类，id为13、19、6的样本被判定为负类（这个判定是根据ground_truth_label判定的）。
除了top-5之外还剩下15个样本:
```
9        0.38        1
16       0.24        1
1        0.23        0
18       0.23        0
5        0.13        0
7        0.12        1
15       0.12        0
10       0.11        0
17       0.1         0
12       0.09        0
20       0.08        1
14       0.07        0
8        0.03        0
11       0.03        0
3        0.01        0
```
这15个样本中的正类为id为9、16、7、20四个样本，负类为id为1、18、5、15、10、17、12、14、8、11、3共11个样本。
那么，计算top5中的Precision=2/5=40%，即对于car这一类别，我们选定的top5个样本中，其中正例有2个，即准确率为40%；
Recall=2/6，即在所有20个测试样本中，共有6个car，但是我们只召回了2个，所以召回率为2/6。

**按照上面的计算方法，我们可以得到一个不同recall阈值下的top1-topN下的Precision和每个recall阈值下的最大Precision:**
```
top-N    Precision    Recall（r）    Max Precision for any Recall r'>=r
1        1/1          1/6            1 
2        2/2          2/6            1  
3        2/3          2/6            1  
4        2/4          2/6            1  
5        2/5          2/6            1  
6        3/6          3/6            3/6 
7        4/7          4/6            4/7 
8        4/8          4/6            4/7 
9        4/9          4/6            4/7 
10       4/10         4/6            4/7 
11       5/11         5/6            5/11 
12       5/12         5/6            5/11 
13       5/13         5/6            5/11 
14       5/14         5/6            5/11 
15       5/15         5/6            5/11 
16       6/16         6/6            6/16 
17       6/17         6/6            6/16
18       6/18         6/6            6/16 
19       6/19         6/6            6/16 
20       6/20         6/6            6/16 
```

**以recall阈值为2/6为例，我们可以得到recall值在2/6的top范围是top2-top5，这其中的最大Precision为1，故recall=2/6时对应的最大Precision为1。其余recall值为1/6，3/6，4/6，5/6，6/6各自也可以求得对应的最大Precision。**

**最后，我们将所有的最大Precision取平均值，即为AP的值。**

# COCO数据集中AP的计算方式
VOC2007/2010数据集在判断预测样本回归框是否正确时用的是IOU>0.5即认为是正样本，但是COCO数据集要求IOU阈值在[0.5, 0.95]区间内每隔0.05取一个值，这样就可以计算出10个类似于VOC数据集中的AP，然后这10个值求平均值即为最后的AP。许多论文中的写法是AP@[0.5:0.95]。

AP衡量的是学出来的模型在每个类别上的好坏，mAP衡量的是学出的模型在所有类别上的好坏。在得到每个类别上的AP后，我们只需要取所有AP的平均值就得到mAP的值。