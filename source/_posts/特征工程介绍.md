---
title: 特征工程介绍
date: 2019-03-12 18:21:30
tags:
- 特征工程
categories:
- 特征工程
mathjax: true
---

# 特征工程介绍
**数据和特征决定了机器学习的上限,而模型和算法只是逼近这个上限而已。**
特征工程是指从原始数据转换为特征向量的过程，这些特征可以很好的描述这些数据，并且利用它们建立的模型在未知数据上的表现性能可以达到最优（或者接近最佳性能）。

# 数据清理

## 缺失值处理
我们首先要计算所有特征的缺失值比例。如果缺失值比例超过阈值（比如25%或15%，这个视情况而定），那么直接删除缺失值；对于缺失比例小于阈值的特征，我们先做数据可视化，观察其分布图，若特征符合均匀分布，则用均值填充，若数据存在倾斜分布，则用中位数填充。如果数据很杂乱，我们也可填充0或随机取值填充；对于样本中缺失值大于等于2的样本数据，我们将其直接删除。
## 异常值和离散点处理
原始数据集中可能包括异常值和离散点。对于异常值数据，我们可以直接采用常识来去除含异常值的样本数据，比如人的年龄不可能小于0岁，也不可能超过200岁；对于离散点数据，我们需要对两两特征做可视化直观地找出离散点，然后删除离散点。
# 数据标准化和离散化
## 数值型数据的标准化
对数值型数据的标准化主要有三种方法:min-Max标准化、z-score标准化、robust标准化。min-Max标准化相当于将整体分布移动到从0开始的位置，但数据分布未变化；z-score标准化将数据变为高斯分布；如果数据中有较多异常值，此时直接做z-score标准化得到的高斯分布将偏离正常范围，我们最好使用robust标准化，使用第一分位数到第三分位数之间的数据产生均值和标准差，然后做z-score标准化。
**注意:**
基于参数的模型或基于距离的模型，都是要进行特征的标准化。比如lr模型这种。基于树的方法可以不进行标准化，例如随机森林，bagging 和 boosting等。
## 类别型数据的离散化
对于类别型数据我们需要先观察其对于模型的贡献大不大，而不是全部变为one_hot编码。假如一个类别型数据有大量取值，但每个取值占总样本数的比例都很小（如5%以下），那么这个类别型数据进行one_hot编码化后每一列one_hot编码特征的方差都很小，对于模型的贡献极其有限。因此这个类别特征我们选择直接删除而不变为one_hot编码。
类别型数据变为one_hot编码时，对于一个有k个取值的类别型特征，我们只需要k-1列one_hot编码就可以表示所有取值。
## 时间型特征的处理
对于类似年-月-日形式的时间型特征，我们需要先将其变为三列年、月、日特征。然后观察我们的数据集标签与这三列特征有无线性关系，如果有，那么保留这些特征，转成one_hot编码；如果没有什么显著的关系，我们可以直接删除这三列特征。
# 特征组合
如果我们知道各个特征的具体含义，我们可以根据其含义来构造新特征，同时适当抛弃一些旧特征。比如金融类数据中利润=成本-支出。我们就可以构造一个利润特征。
如果我们不知道各个特征的具体含义，我们可以用多项式变换构造一系列特征。构造后的特征我们再使用特征选择方法挑选出对模型贡献大的特征，其余特征抛弃。
**常见的就是二项式变换:**
假如一个输入样本是２维的，形式如[a,b]，则多项式变换后的特征有（其实就是（a+b）*（a+b）的每一项加上原始的a和b）:
a，b，a平方，b平方，2ab。
更多项的多项式变换也类似。
# 特征降维
经过上面处理后的特征数据量大大增加，但不是每一项特征都对模型有用。我们可以使用特征降维方法PCA和LDA压缩数据集规模，并保留对模型最有用的特征。在进行特征降维前，我们必须把数值型特征标准化，把类别型特征归一化。
PCA方法是一种无监督学习方法，把原始n维数据x变换到一个新的坐标系统中，使得任何数据投影的第一大方差在第一个坐标（称为第一主成分）上，第二大方差在第二个坐标（第二主成分）上，依次类推。最后我们只保留前k个主成分，这前k个主成分的方差是最大的，因此对模型的贡献最大。而抛弃的n-k个维度对于模型的影响很有限，这样我们就降低了运算量。
LDA方法是一种有监督学习方法，需要使用特征x和标签y。LDA的目的是使投影后类内方差最小，类间方差最大。将数据在低维度上进行投影，投影后使每一种类别数据的投影点尽可能的接近，而不同类别的数据的类别中心之间的距离尽可能的大。
# 数据采样
数据采样主要是为了处理样本不均衡问题的。有时我们的数据集中正负样本个数差距很大，而大多数模型对正负样本比是敏感的（比如逻辑回归），所以，需要通过数据采样，来使数据正负样本均衡。
**在处理样本不均衡问题时，主要分为两种情况:**
正负样本个数差距很大，并且同时正负样本个数本身也很大，这个时候可以采取下采样方法，即对训练集对训练集里面样本数量较多的类别（多数类）进行欠采样，抛弃一些样本来缓解类不平衡；
正负样本个数差距很大，并且同时正负样本个数本身比较小，这个时候可以采取上采样方法，即对训练集里面样本数量较少的类别（少数类）进行过采样，合成新的样本来缓解类不平衡。最典型的算法就是SMOTE算法。
# 特征选择
当上面的工作都完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。
**根据特征选项的形式，可以将特征选择方法分为三种:**
Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，排序留下Top K个特征；
Wrapper：包装法，根据目标函数对每个特征的评价，每次选择若干特征，或者排除若干特征；
Embedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。
## 过滤法
**方差选择特征法:**
使用一个阈值，方差大于阈值的特征保留。
**卡方检验法:**
$$
X^{2}=\sum \frac{(\text { observed }-\text { expected })^{2}}{\text { expected }}
$$
observed即实际该类样本数，expected即理论上该类样本数（全样本中该类的比例乘以该特征总样本数）。
举例：
```
# 男 女 合计
# 化妆 15(55) 95(55) 110
# 不化妆 85(45) 5(45) 90
# 合计 100 100 200
```
假设H0：化妆与否与性别无关。 如果化妆与否与性别没有关系，四个格子应该是括号里的数（期望值，即理论上该类的样本数，假设化妆/不化妆和性别无关，那么男和女的样本数应各占50%）。 
自由度=（特征数-1）*（类别数-1）。 
这里化妆/不化妆是特征，男/女是类别。
查表自由度为1，分位数为0.001时的卡方临界值是10.828，卡方值越大，分位数越小。
计算卡方值: 
 $$ X^{2}=\frac{(15-55)^{2}}{55}+\frac{(95-55)^{2}}{55}+\frac{(85-45)^{2}}{45}+\frac{(5-45)^{2}}{45}=129.3>10.828 $$ 
129.3>10.828，即有小于0.001的概率假设H0成立，换句话说就是化妆与性别相关成立的概率p>0.999,即99.9%。 
**皮尔森相关系数法:** 
即两个变量X、Y之间的协方差和两者标准差乘积的比值。
$$ \rho_{X, Y}=\frac{\operatorname{cov}(X, Y)}{\sigma_{X} \sigma_{Y}}=\frac{E\left[\left(X-\mu_{X}\right)\left(Y-\mu_{Y}\right)\right]}{\sigma_{X} \sigma_{Y}} $$ 
对于数据集样本的皮尔森相关系数: 
$$ r=\frac{\sum_{i=1}^{n}\left(X_{i}-\overline{X}\right)\left(Y_{i}-\overline{Y}\right)}{\sqrt{\sum_{i=1}^{n}\left(X_{i}-\overline{X}\right)^{2}} \sqrt{\sum_{i=1}^{n}\left(Y_{i}-\overline{Y}\right)^{2}}} $$ 
上面的i到n指从第一个样本到第n个样本，X和Y分别指一个特征。
皮尔森相关系数反映了两个变量的线性相关性的强弱程度，r的绝对值越大说明相关性越强。 
当r>0时，表明两个变量正相关，即一个变量值越大则另一个变量值也会越大；
当r<0时，表明两个变量负相关，即一个变量值越大则另一个变量值反而会越小；
当r=0时，表明两个变量不是线性相关的（注意只是非线性相关），但是可能存在其他方式的相关性（比如曲线方式）；
当r=1和-1时，意味着两个变量X和Y可以很好的由直线方程来描述，所有样本点都很好的落在一条直线上。
## 包装法
根据目标函数对每个特征的评价，每次选择若干特征，或者排除若干特征。也可以将特征子集的选择看作是一个搜索寻优问题，生成不同的组合，对组合进行评价，再与其他的组合进行比较。
这样就将子集的选择看作是一个优化问题。
**递归特征消除法:**
假如我们选择SVM-RFE算法，进行多轮训练。在第一轮训练过程中，会选择所有特征来进行训练，继而得到了分类的超平面w*x+b=0。如果有n个特征，那么SVM-RFE会选择出w中分量的平方值最小的那个序号i对应的特征，将其删除；在第二轮的时候，特征数就剩下了n-1个，继续用这n-1个特征和输出值来训练SVM；同样的，继续去掉w中分量的平方值最小所对应的特征。以此类推，直到剩下的特征数k满足我们的要求为止。
我们在sklearn中实际应用SVM-RFE算法时，可以通过学习器返回的coef_属性或feature_importance_属性来获得每个特征的重要程度，然后从当前的特征集合中移除不重要的特征。在特征集合上不断重复上述过程，直到最终达到所需要的特征数量k为止。
## 嵌入法
使用某些机器学习的算法和模型，数据集选择完整数据集，进行训练，得到各个特征的对模型的贡献度，根据贡献度从大到小选择特征。
**基于惩罚项的特征选择法:**
在线性模型中，越是重要的特征在模型中对应的系数就会越大，而跟输出变量越是无关的特征对应的系数就会越接近于0。在噪音不多的数据上，或者是数据量远远大于特征数的数据上，如果特征之间相对来说是比较独立的，那么即便是运用最简单的线性回归模型也一样能取得非常好的效果。
L1正则化将系数w的l1范数作为惩罚项加到损失函数上，由于正则项非零，这就迫使那些弱的特征所对应的系数变成0。因此L1正则化往往会使学到的模型很稀疏（系数w经常为0），这个特性使得L1正则化成为一种很好的特征选择方法。
L2正则化将w的L2范数添加到了损失函数中。由于L2惩罚项中系数是二次方的，这使得L2和L1有着诸多差异，最明显的一点就是，L2正则化会让系数的取值变得平均。对于关联特征，这意味着他们能够获得更相近的对应系数。
我们可以选择带L1惩罚项的逻辑回归作为基模型,可以输入一个阈值过滤掉权值系数低的特征。如果使用参数惩罚设置为L1，则使用的阈值为1e-5，否则默认使用mean。
**基于树模型的特征选择法:**
随机森林模型也可以用来进行特征选择。它提供了两种特征选择的方法：平均不纯度减少（mean decrease impurity）和平均精确率减少（mean decrease accuracy）。平均不纯度减少即对于一个决策树森林来说，可以算出每个特征平均减少了多少不纯度，并把它平均减少的不纯度作为特征选择的值。平均精确率减少即打乱每个特征的特征值顺序，并且度量顺序变动对模型的精确率的影响。对于不重要的变量来说，打乱顺序对模型的精确率影响不会太大，但是对于重要的变量来说，打乱顺序就会降低模型的精确率。