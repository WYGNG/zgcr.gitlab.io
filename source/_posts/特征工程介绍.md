---
title: 特征工程介绍
date: 2019-03-12 18:21:30
tags:
- 特征工程
categories:
- 特征工程
---

# 特征工程介绍
特征工程是指从原始数据转换为特征向量的过程，这些特征可以很好的描述这些数据，并且利用它们建立的模型在未知数据上的表现性能可以达到最优（或者接近最佳性能）。特征工程是机器学习中最重要的起始步骤，会直接影响机器学习的效果，并通常需要大量的时间。典型的特征工程包括数据清理、特征提取、特征选择等过程。
# 特征处理
## 数据清理
采集到的数据中可能包含异常值数据、缺失值数据，我们要根据常识或其他方法去除这部分数据。
对于异常值数据，我们可以直接采用常识来去除，比如人的年龄不可能超过200岁。
对于缺失值数据，如果所有样本中的的缺失值比例很高(比如大于25%)，可以直接去掉；如果缺失值不是很多(小于5%)，若特征符合均匀分布，则用均值填充，若数据存在倾斜分布，则用中位数填充。如果数据很杂乱，我们也可填充0或随机取值填充。
## 数据采样
数据采样主要是为了处理样本不均衡问题的。有时我们的数据集中正负样本个数差距很大，而大多数模型对正负样本比是敏感的（比如逻辑回归），所以，需要通过数据采样，来使数据正负样本均衡。
**在处理样本不均衡问题时，主要分为两种情况:**
正负样本个数差距很大，并且同时正负样本个数本身也很大，这个时候可以采取下采样方法，即对训练集对训练集里面样本数量较多的类别（多数类）进行欠采样，抛弃一些样本来缓解类不平衡；
正负样本个数差距很大，并且同时正负样本个数本身比较小，这个时候可以采取上采样方法，即对训练集里面样本数量较少的类别（少数类）进行过采样，合成新的样本来缓解类不平衡。

## 数值型/类别型/时间型数据的处理
**对数值型数据:**
我们需要计算对应特征的最大值，最小值，平均值，方差等，从而对数据进行更好的分析；
对于不同尺度的特征值，我们要对它们进行数据归一化，通常使用z均值标准化，将数据值都缩放到（0，1）区间；
对于年龄等区间段进行分类的连续值数据，我们要将其离散化。
**对于类别型数据:**
使用one_hot编码来区分类别。
**对于时间型数据:**
对于时间型数据来说，即可以把它转换成连续值，也可以转换成离散值。
# 特征降维
经过上面特征处理后的数据可能包含了过多的特征，而特征特征维数越高，模型越容易过拟合，此时更复杂的模型就不好用。
相互独立的特征维数越高，在模型不变的情况下，在测试集上达到相同的效果表现所需要的训练样本的数目就越大。特征数量增加带来的训练、测试以及存储的开销都会增大。
因此，我们要进行特征降维工作，最常用的方法就是PCA。
# 特征选择
当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。
**根据特征选项的形式，可以将特征选择方法分为三种:**
Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，排序留下Top K个特征；
Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征；
Embedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。
# 特征组合
特征组合也称为特征交叉，指通过将两个或多个输入特征相乘来对特征空间中的非线性规律进行编码的合成特征。
**常见的特征组合方法有:**
[A X B]：将两个特征的值相乘形成的特征组合。
[A x B x C x D x E]：将五个特征的值相乘形成的特征组合。
[A x A]：对单个特征的值求平方形成的特征组合。

借助特征组合，线性学习器可以很好扩展到大量数据，并有助于构建复杂模型解决非线性问题。